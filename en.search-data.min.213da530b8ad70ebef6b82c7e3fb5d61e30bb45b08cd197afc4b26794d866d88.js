'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/case-study-amazons-dynamodb/','title':"14. Case study: Amazon's DynamoDB",'section':"Docs",'content':"14. Case study: Amazon\u0026rsquo;s DynamoDB #  论文下载 #  https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf\n可用性(Availability) #  我们之前对可用的定义是所有的请求都会收到回复。这里做一些修改，变为所有的请求在一定的时间内都会收到有意义的回复。\n如果用户想更新自己的购物车，即使存在网络分区，也尽管让他们继续更新而不是等待备份节点的确认。这样的方式会导致一些数据的丢失和不一致，但是是可以保证高可用。\n最终一致性(Eventual Consistency) #  Dynamo论文中并没有阐明是否达成了强收敛，以及也并没有提到强最终一致(String eventual consistency)，可能是因为Dynamo发表的年份早于这个术语的诞生。\n向量时钟(Vector Clocks) #  论文中有提及向量时钟的用途：用来追踪对象(object)的历史。对于每一个事件都会分配一个逻辑的时间戳，当冲突产生的时候Dynamo会使用向量时钟去处理冲突。但是向量时钟不一定每次都能奏效，原因我们也多次讨论过，向量时钟不能处理当事件是独立或者并发的情形。此时要不使用物理时钟(physical timestamps)或者将冲突留给客户端处理。客户端的开发者对冲突的解决的策略可能会更加了解些。\nDynamo允许你提供自己的冲突处理机制，如果不提供的话，则采用最后一次写(last write wins)。\n我们使用Lamport diagrams可以从全局的角度看一个系统，但是在真实额系统中并不能画出这样的图去发现备份间的冲突。Dinamo有一些机制去检测和解决这样的冲突，其中一个叫做逆墒(Anti-entropy)。\n逆墒(Anti-entropy) #  逆墒是一种用来保证备份间一致的协议。其工作方法为： 一个节点每固定的时间随机地联系其他的节点并询问其状态。\n论文中还提到了gossip，一种用于节点间对具体Key-Value对的查询的通信方式。Dynamo使用gossip去查询其他节点的路由表以知道系统中活跃节点。系统中除了具体的数据，活跃的节点信息也很重要，gossip在dynamo中就是用来做节点是否失联的追踪的。\nAnti-entropy是用来处理具体数据的冲突的，而gossip则是用来使系统内活跃节点信息一致。\n这里有个问题是上述的两种情形有什么差别？尽管概念上讲两者很像，但是数据的差别可能会很大，而内活跃节点信息会很小。如果询问两个节点间的数据差别，则有可能要发送大量的数据包。\n所以，Dynamo中为了对比两个节点中的数据差别，不会发送整个状态，而是使用了一种哈希树来减小数据传送的代价。\nMerkel trees #  {% embed url=\u0026ldquo;https://brilliant.org/wiki/merkle-tree/\u0026quot; %}\n我们需要解决的问题是不想把一个节点的状态完整的发送给另一个节点。但是如果两个节点不一致的话，该如何找出不一致的数据呢？\n如果两个节点的状态的root hash值不一样，则递归地到左右两棵子树中查找，知道找到了叶子节点具体的数据块中不一样的地方。\nQuorum Consistency #  在经典的主备模型中，客户端会只会和特定的节点进行通信。但是在Dynamo中，客户端可以和任意的一个节点通信，每个节点的角色是一样的。此时的客户端有点像是在充当主节点，它需要和这些备份节点直接通信，但是这些节点中有多少是客户端可以直接通信的呢？这个在Dynamo中是可以配置的。\n N表示备份节点数量（比如3个） W表示备份节点中需要对写请求响应ACK的数量。（Write quorum) R表示备份节点中需要对读请求响应ACK的数量。（Read quorum)  W和R最多是N个，最少呢？让我们尝试下不同的组合，比如W是3，R是1，这样的组合可以保证强一致么？显然，每次写请求需要3个节点确认，那么必然是能保证强一致的，但是缺点是不能容错（fault tolerant），只要有一个节点故障了，或者出现了网络分区，那么就不能得到3条写确认。这种方式叫做WARO(Write All Read one)，尽管读操作可用性可以提高，但是写操作的可用性却降低了。\nDynamo 论文中的建议是设置R = 2，W= 2。\n上面的例子中一个备份认为X等于4，一个备份认为X等于5，此时我们就知道不一致的情况发生了从而需要我们采取一些措施。比如重新请求，此时三个备份的数据已经一致。\n一般来说，我们需要保证R + W \u0026gt; N。\n"});index.add({'id':1,'href':'/docs/eventual-consistency/','title':"13. 最终一致性(eventual consistency)，CAP理论",'section':"Docs",'content':"13. 最终一致性(eventual consistency)，CAP理论 #  最终一致性(eventual consistency) #  需要实现强一致(strong consistency)是很难的，比如想想我们之前多次提到的total order anomaly.\n这是一个total order anomaly, 那它会是一个causal anomaly么？我们从vector clocks里面是否可以找到什么答案呢？\n显然， 我们并不能从向量时钟里判断是否为causal anomaly因为C1和C2两个发送消息的时间是互相独立的。\n如果我们需要使这些备份是一致的，该怎么办呢？\n一个方案是使用共识算法，我们在上一篇文章讨论过。但是共识算法带来的问题是会发送很多的消息而导致代价很高以及共识算法不一定能终止(Terminate)。所以我们会更加倾向于找到一些方法使得这些操作到达备份节点的顺序不再重要。\n所以上面的例子该做如何修改才能使得操作顺序不重要呢？一个做法是C1只更新X的值，C2只更新Y的值。\n那这样可以达到强一致(Strong Consistency)么？并不能，回顾一下强一致的概念，是要客户端Client并不能意识到多个节点的存在。如果C1只允许更新X的值，C2只允许更新Y的值，这样可以达到强一致么？答案是不能。有一种可能是C2的消息比较缓慢，导致客户端在读取的时候有可能处于一个中间状态，即某个时刻R1和R2的的Y的值是不一致的。但是似乎目前的做法相较于文章开始时的做法已经有了提升。其实这种做法能够使得一段时间后，备份节点上面的数据最终会一致，这叫做最终一致性(Eventual Consistency).\n最终一致性(Eventual Consistency)的非正式定义是当所有的客户端不再发送更新的时候，备份最终会达成一致。\n最终一致性是符合safety还是liveness呢？显然是liveness。它不可以在有限的时间或者执行后被证明可以被违背。\nCausal consistency/Strong Consistency/FIFO Consistency都是符合Safety性质，而Eventual Consistency符合liveness性质。\n很多的一致性模型都属于Safety性质是因为可以在有限的执行后可以判定出是否违背。\n另一个可能有点让人混淆的概念是强收敛(Strong Convergence)。强收敛(Strong Convergence)是符合Safety性质的。\n强收敛(Strong Convergence): 所有交付了相同集合更新的备份应该有相同的状态。\n在上图标亮的位置，R1和R2都交付了相同的更新，所以这时我们可以说他们是强收敛的。\n对于相同的更新集合，不管他们是以什么样的顺序到达的备份节点，只要他们被交付后使得所有的备份节点的状态相同，我们就可以说是强收敛的。所以有时会听到有人说系统是强最终一致的(strong eventual consistency)，实际上是将最终一致(eventual consistency)和强收敛(strong convergence)结合在了一起。强最终一致是Safety和Aliveness的结合。\n看到这里你可能会觉得强收敛还挺容易实现的，其实不然。现实的系统中我们不能让一个客户端只更新X，一个客户端只更新Y，而是应该让不同的多个客户端去写相同的Key。那如何让不同的客户端同时写X但还能保证强收敛呢？\n当然你可以使用共识算法，保证只有所有的备份节点都达成共识了再交付这些更新，但是这样做的话就太重了（需要发很多的消息）。\n相较于使用共识算法，有一种轻量级的实现。\n我们可以让X不再被存储为单个值，而是一系列的值。比如上图中X最后就分别存储了{1,2}和{2,1}，因为集合是无序的，所以最终的两个备份最终的状态是相同的。\n那现在唯一的问题就是达成强收敛后客户端读取了。此时客户端可能需要做一些冲突处理，这个依赖于具体的业务，客户端的操作会变得有些复杂，但是我们的确达到了强收敛，但是引入了新的需要妥协的工作：客户端进行冲突处理。\n一个例子就是比如用户在手机上往Amazon的购物车里放了一本书，又在电脑端往购物车里放了一包薯片。显然，这种业务场景下客户端应该保留书和薯片而不是移除其中任意的一种。\n分区(Partitions)\n我们可以想像一个场景： 网络中的一部分节点和另外一部分节点之间本应该是可以正常通信的，但是由于一些原因导致他们这些分组间不能通信。这就是网络分区(Network Patition)。\n另外的一个场景是一个客户端可以同这两个分区通信，但是分区间不能通信。更罕见的场景是分区1可以发送消息给分区2，而分区2却不能给分区1发送消息。网络分区是分布式系统中不可避免的现象。\n我们可以用之前讨论过的错误模型中的消息丢失(omission model)模型来理解网络分区。\n当网络分区产生的时候，消息无法从一侧发送到另一测。\n不要和数据分区(Data Partitioning）混淆。数据分区(Data Partitioning）也叫Sharding，是指当一台机器无法存放所有的数据的时候，将数据分布在多个机器上。我们需要决定把数据放在哪些机器上。这个在后面的Amazon\u0026rsquo;s Dynamo DB的case study中会讨论。\n可用性(Availability) #  另一个术语就是可用性(availability)。简单来说就是任何一个请求都可以获得回复。\n比如在之前的主备模式(primary backup replication )的系统中，可以保证每一个请求都能收到回复么？答案是不能，因为主节点收到请求后，需要保证所有的备份节点都确认后再进行给客户端的答复，但是如果备份节点出现失败不能进行确认，那么主节点也就不能发送回复给客户端。\n可能有同学会想，那可不可以主节点收到请求后先进行回复，再通知备份节点呢？\n上面做法的问题是，比如出现了网络分区导致主节点不能跟备份节点通信了，那么你不知道网络分区什么时候被修复，很有可能导致很长时间主备不能通信，从而导致主备之间的数据不一致。\n所以如果想追求高可用的话，就会牺牲掉一些一致性。\n当然我们之前讨论的主备系统是不是发生上面的情形，而是\n这样的系统优先考虑了一致性，但是却牺牲了可用性。\n所以在进行系统设计的时候，可用性和一致性不能同时满足，需要作trade off的。比如Amazon的DynamoDB则是牺牲一致性和优先保证可用性。\nCAP理论 #  CAP分别代表Consistency，Availability和Partition Tolerance。一个系统不能同时保证一致性，可用性和分区的容错性。我们在做系统设计的时候，需要考虑更多的一致性还是更多的可用性。\n"});index.add({'id':2,'href':'/docs/replications-and-consistency/','title':"11. 冗余备份(Replications)与一致性(Consistency)",'section':"Docs",'content':"11. 冗余备份(Replications)与一致性(Consistency) #  为什么做备份 #  首先我们思考一个问题，为什么要做备份？\n 减少数据丢失。其实是增加容错的能力，多份备份的容错能力强于一份数据。 数据的本地化(Data Localiity)。用户优先访问离他物理距离近的备份加快访问速度。 可拓展性(Scalability)。可以支持更多的请求访问，或者说让更多的节点分担工作。  对数据做备份的好处是显而易见的，但是做备份是有代价的：\n 我们需要保证备份间数据的一致性，这点下面会重点讨论。 昂贵的。我们需要购买更多的服务器或者云服务去支持备份。  一致性(Consistency) #  我们再看次违背total order delivery的例子：\nR1， R2两个备份保存的X值最终没有一致，这就是违背了一致性原则。\n其实一致性不光出现在多个备份时，单个节点时一样存在一致性的问题，比如下面的例子：\n上面的两个例子中，都只有一个备份R(严格意义上讲，不能叫做备份)，两个客户端C1，C2。C1和C2同时发起修改X的请求，但是却导致了两种结果。\n那这两个例子是否违背了total order delivery呢？显然没有，total order delivery （TO）是要求多个进程间的交付顺序一致，而我们这里两个例子都只有一个进程进行消息交付。\n上面的这种现象我们称作违背了决定论（determinism）。determinism是用来描述系统多次运行的结果的。我们发现TO是可以描述一次系统运行的性质，而determinism是可以描述多次运行。这里很晦涩，可以略过不做深入理解。\n所以，如果只有一个节点，一个备份的情况下，不违背total order delivery很简单，那么如果我们能在有多个备份的情况下，使得客户端意识不到存在多个备份还继续保持total order delivery那真的太美妙了。\n这里就可以给出强一致(Strong Consistency)的一个非正式的定义：如果客户端无法感知到提供服务的多备份的系统是有多个备份的，那么这个系统就是强一致的。\n那么回到多备份的情况下，一个很容易出现的错误如下：\n客户端发送的设置X的值请求被发送给R1，但是查询X的请求却发送给了R2。这种错误我们称作读自己写异常(Read your Write Anomoly)。\n有同学可能会指出，R1在接收到C1的消息后，可以转发给R2。这是一种解决方案，但是还是可能有问题。\n再举个例子：\n假设一个用户有两个客户端C1，C2，同时银行有两个备份R1，R2，C1发起存款50后，又发起提现40。R1转发了这两条消息给R2，但是最终交付的时候是提现40先进行了交付。C2查询银行余额时可能就会出现-40元。\n那么上面画圈的部分是发生了什么异常呢？这个我们之前学习过，是违背了FIFO delivery。\n在一致性的概念里，上面的例子违背了FIFO 一致性。\nFIFO Consistency：一个进程P完成的写操作顺序，其他所有进程接收到写顺序应该和P保持一致。\n我们继续看银行的例子：\n我们看下事情经过：用户通过客户端C1存款50， 再通过客户端询问余额，答复还有余额50，然后发起取现40，却发现完成不了。\nC2实际上没有做错任何事情，问题在于备份2并没有看到完整的事件因果。这种一致称作因果一致性。\nCasual Consistency: 所有的写操作如果存在因果关系，那么所有的进程都应该看到这样的因果关系。\n我们来总结下一致性的一些层级：\n不同的系统会采取不同的一致性，有时候我们会采取相对弱的一致而不是一致性越强越好，这个在后面我们会探讨。\n实现强一致 #  第一种强一致的实现方式叫做主备模式(Primary-backup replication)。这种模式下只有主节点进行写操作，同时将写操作广播到备份节点。主节点还进行读操作。\nC1的请求发送到Primary后，Primary将消息广播到B1和B2，B1 B2分别ACK后，Primary再将X的值写入（图中标黄的地方，这个点称作提交点，Commit point），最后Primary发送ACK给C1。\n最后C1询问X的值，Primary作出回应。\n这个算法很简答，上个世纪70年代就被提出来了。我们可以观察到在任意时刻，备份的数据是不老于主节点的，甚至比主节点还要新。\n那这个算法的缺点呢？\n首先，主节点是瓶颈。\n我们再对照着文章开始提出的为什么要备份，看看这个算法解决了哪些？\n 减少数据丢失。其实是增加容错的能力，多份备份的容错能力强于一份数据。 数据的本地化(Data Localiity)。用户优先访问离他物理距离近的备份加快访问速度。 可拓展性(Scalability)。可以支持更多的请求访问，或者说让更多的节点分担工作。  减少数据丢失, 增加容错的能力显然是可以做到的，任一时刻如果主节点崩溃了，任一一个备份节点可以做主节点。\n数据的本地化呢？并没有，因为任何地区访问的都是主节点。\n可拓展性呢？或者称作水平拓展，其实也没有，所有的读写请求在由主节点承担了。\n那能不能改变一下上面的算法，使得其表现的更好？有一个思路就是能不能实现读写分离。一个节点负责写操作，一个节点负责读操作。\n下面的算法称作链式复制(Chain Replication)，Primary节点作为写节点，链式转发到各个备份节点，最后的备份作为读节点。\n这篇论文发表在 2004， 叫做 Chain replication for supporting high throughput and availability， 链式复制以支持高吞吐和高可用。\nPrimary节点又叫做头节点(head), 而最后一个备份叫做尾节点(Tail)。\n相较于主备模式只有一个节点进行读写操作，链式备份有两个节点分别进行读写，所以理论上的吞吐量会增加。吞吐(Throughput)是指单位时间内完成的动作。\n但是我们看下另一个指标，延时(latency)：延时是指动作发起到完成后的时间。\n在主备模式中，由发送消息给主节点+主节点广播(并行的)+最长的ack+回复消息给客户端，4条消息完成，但是在链式备份中，由发送消息给头节点+n条链传递+尾节点ack，主要取决为备份的数量，所以链式备份的写延时会很有可能高于主备模式。\n但延时在两种模式下是几乎一样的。\n"});index.add({'id':3,'href':'/docs/implementation-of-reliable-delivery/','title':"10. 实现可靠消息交付",'section':"Docs",'content':"10. 实现可靠消息交付 #  我们先回顾下可靠消息交付的定义：\nP1发送了消息m到P2，那么P1或者P2其中的任意一个崩溃了，P2最终还是会交付消息m。\n上面的定义可能会有不同的版本，比如：\nP1发送了消息m到P2，那么P1或者P2其中的任意一个崩溃了，并且消息没有丢失，P2最终还是会交付消息m。\n其实上面的两种定义的差别在于我们讨论的错误模型，如果是消息丢失模型的话，那我们就要考虑消息丢失的情况，如果只考虑崩溃模型的话，那么则不需要考虑消息丢失。\n那么消息丢失模型和崩溃模型哪种场景会更加实际一些呢？\n显然是消息丢失模型，我们在上篇文章中提到过，崩溃模型只是消息丢失模型的一个子集。\n那如果把模型切换到拜占庭模型呢？上面的可靠消息交付的定义是不是也要发生变化了呢？是的，那就要加上发送的消息没有被恶意地操控。那这样对可靠消息交付的定义太过于复杂了，至于这个定义如何简化使其更加通用，我们会在文章后面再次探讨。\n实现可靠消息交付 #  比如Alice需要给Bob发送消息，Alice如何保证Bob已经交付了消息了？\n有一种策略就是Alice将要发送的消息放入缓冲区，然后不停地给Bob发送这条消息，直到Bob确认(ACK)了这条消息已经交付，Alice从缓冲区中移除这条消息。\n但是Alice给Bob重复发送了多次相同的消息，或者说Bob接收到了多次相同的消息，会有什么影响吗？\n这个需要分情况讨论。\n上面的例子中，Alice发送了多条消息将X设置为5，尽管Bob多次接收到这条消息，但是最终的X是正确的，等于5。\n上面的例子中，Alice只想将 X+1，但是由于重复发送，导致最终X被加了两次。这个不是我们期待的。\n上面的第一种操作我们称作幂等的(idempotent)，幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。\n如果想保证消息被可靠地交付了，一个理想的做法就是尝试多次发送，但是不是所有的消息操作都是幂等的。但是简单来说，如果一个消息是幂等的，那么我们可以简单地通过上述多次发送的方式保证消息的可靠交付。\n可靠交付有时又称作至少一次交付(at least once delivery)，也就是说一条消息至少要被交付一次。\n与之对应的就是至多一次交付(at most once delivery)，消息只会被发送一次，那也就最多被交付一次。\n可能有同学会问了，能不能做到恰好一次交付(exactly once delivery)呢？\n实际上，如果有宣称其系统可以做到恰好一次交付，那是基于要交付的消息是幂等的前提。\n实现可靠广播(Reliable Broadcast) #  广播(Broadcast)这个词我们已经提到过多次，这里做更多的介绍。\n单播(UniCast)：一个进程给另一个进程发送消息。一个发送方，一个接收方。是一对一的关系。\n广播(Broadcast)：一个进程给其他所有进程发送消息。一个发送方，剩下的都是接收方。\n多播(multicast)：一个进程给多个进程发送消息。一个发送方，多个接收方。\n单播和广播我们之前都有见过，那么多播模型呢？\n上面的例子是来自之前讲的违背Total Order Delivery的例子，C1 给 R1和R2发送消息，但是不给C2发送，这就是多播。\n这里不会讨论单播，因为我们做的基本前提就是一个进程给另一个进程发送消息。\n那我们该如果使用单播实现广播呢？\n广播是一个发送方给剩下的所有进程发送消息，那我们可以从这个发送发发起多次单播分别给其他进程发送消息，这样就实现了广播。\nAlice要对Bob和Carol进行广播，尽管在Lamport图中只有一个事件，实际上是通过两次单播实现的。\n既然是两个独立的单播发送事件，那问题就要出现了。\n我们先给出可靠广播交付的定义：如果一个正确的进程广播了消息m，那么所有的正确的进程都会交付m。\n什么叫做正确的进程呢？这个需要看我们所处的错误模型，如果是拜占庭模型，那么正确的进程是没有恶意的进程，如果是崩溃模型，那么正确的进程是没有崩溃的进程。\n回答本文最开始关于可靠消息交付更加普遍的定义：如果一个正确的进程P1发送消息给正确的进程P2，并且消息m没有丢失，那么P2最终一定会交付m。这里的定义还是需要保证m没有丢失，因为即使P1, P2是正确的，消息也有可能在传输过程中丢失。\n回到可靠广播交付，我们看下下面的例子。\nAlice 广播了消息m，Bob成功进行了交付，但是Alice在接收到消息后崩溃了，没有对m进行交付。这个例子违背可靠广播的定义了吗？答案是NO，因为Carol崩溃了，不符合“正确进程\u0026quot;的前提。\nAlice想要广播消息m，在单播了给Bob的消息后，进程崩溃，导致给Carol的消息没有播出。这个例子违背可靠广播的定义了吗？答案是Yes，因为在Alice开始时Alice是正确的进程，但是中途崩溃导致广播没有完成。\n既然在广播的过程中任意的一个进程都有可能崩溃，那该如何实现可靠广播呢？\n假设我们制定这样的一个协议：一个进程在接收到消息后，里面广播这条消息，然后进行交付。\n即使Alice仅仅单播了Bob，Carol也会接收到消息。\n如果Bob接收到消息后还没来得及广播就崩溃了呢？\n实际上上面的例子并不违背可靠广播的定义，因为Bob没有交付这条消息，Carol也没有！\n下面给出可靠广播的具体算法：\n 所有的进程维护一个已经交付消息的集合。 当进程P想要广播消息m  P将m单播到其他素有的进程（除了自己） P将m加到自己已经交付消息集合里面   当P1接收到来自P2的消息时：  如果m在已经交付消息集合里面，丢弃这条消息。 否则，将m单播到其他素有的进程（除了自己P1和P2） 将m加到自己已经交付消息集合里面    注意到，上述算法通过发送消息给消息源进程，可以减少一些消息量。\n我们可以发现要实现可靠的广播采取的方式是对消息进行冗余备份从而进行容错处理，这种思路在分布式系统的设计中很常见，下一篇文章将会介绍这种思路的另一个应用：冗余备份。\n"});index.add({'id':4,'href':'/docs/reliable-delivery/','title':"9. 可靠消息交付",'section':"Docs",'content':"9. 可靠消息交付 #  可靠消息交付 #  P1发送了消息m到P2，那么P1或者P2其中的任意一个崩溃了，P2最终还是会交付消息m。\n我们再回顾下第一篇文章中的例子，\n上面的传输，可能会发生哪些错误？\n m1发送的消息丢失 m1发送的消息传输得太慢 m2崩溃了 m2回复的太慢 m2回复的消息传输的太慢 m2回复的消息丢失 m2撒谎了  错误分类 #  传统上，我们可以将错误分为以下几类\n 丢失错误(Omission Fault)，消息丢失。 时机错误(Timing Fault)，消息处理或者传送得太慢。 崩溃错误(Crash Fault)，进程崩溃。 拜占庭错误(Byzantine Fault)。恶意或者随意的消息。  我们再对照下上面的例子中7个错误，他们可与你被划分为：\n m1发送的消息丢失。丢失错误 m1发送的消息传输得太慢。时机错误。 m2崩溃了。崩溃错误。 m2回复的太慢。时机错误。 m2回复的消息传输的太慢。时机错误。 m2回复的消息丢失。丢失错误。 m2撒谎了。拜占庭错误。  错误类型分层 #  上面的四种错误自1990年代开始一直在使用，我们给他们更加精确的定义，然后试着对其进行分层。\n 丢失错误(Omission Fault)，消息丢失。（进程无法去接收或者发送消息） 时机错误(Timing Fault)，消息处理或者传送得太慢。（进程回复得太慢） 崩溃错误(Crash Fault)，进程崩溃。（进程停止发送或者接收消息） 拜占庭错误(Byzantine Fault)。恶意或者随意的消息。（进程会恶意或者不按规则随意地进行响应）  我们试着对上面的4中错误进行分层。\n假设我们定义了协议X能够容忍崩溃错误，协议Y可以容忍消息丢失错误，那么协议Y可以容忍崩溃错误吗？\n实际上崩溃错误是可以当作一种特殊的消息丢失错误，进程崩溃了，消息也自然无法发送和被接收。所以一个进程如果能够对消息丢失容错，那么也能对崩溃容错。\n如果一个协议可以进行拜占庭容错，那么能够容忍其他的错误么？\n我们可以设想一下，如果拜占庭错误是进程故意假装崩溃，那这个错误就是崩溃错误。如果进程假装没接收到消息，那么就是消息丢失错误。如果进程故意拖延不进回复消息，那就是时机错误。所以，一个协议如果可以进行拜占庭容错，那么也可以进行其他错误的容错。\n所以，最终的关系如下。\n你可能发现了，我们好像还没讨论时机错误。我们后面不会过多讨论时机错误，是因为我们的网络模型是基于异步网络，异步网络在第一篇文章中有所介绍。因为是异步网络模型，所以不存在一个固定的时间N保证消息一定能够送达，因此也无法设计一个协议保证进行时机容错。\n但是我们还是可以简单地讨论下，当进程崩溃或者消息丢失的时候，其实都是一种特殊的时机错误。所以如果能够对丢失或者崩溃容错，那也能对时机错误容错。\n实际上，我们还可以对拜占庭错误进行细分。比如我们可以利用一些认证或者机制，比如checksum，对接收到的消息进行检测，如果发现消息被篡改了，我们可以直接丢弃到这条消息。\n错误模型 #  错误模型是对一个系统可能出现的错误种类的描述。\n比如我们可以说我们正在处理崩溃模型，那就意味着我们的系统需要对崩溃错误进行容错，同理，丢失错误模型需要对消息丢失进行容错。\n对消息丢失进行容错是很难做到的一个模型，甚至可以说无法做到的一个模型。有一个著名的问题就是对消息丢失模型的描述：two generals problem，两军问题：\n两个军队分在两处，如果想要打败敌人，需要两个军队一起出兵才能成功。两个军队间是通过信使进行消息传递，比如军队A决定晚上发起进攻，那么信使就会送信去军队B，B确认后，信使再回到A，告诉A军队B军队已经知情。但是问题是信使可能会在中途被俘虏，那么消息就不能到达。这个问题很像TCP建立可靠连接的模型，尽管TCP采用了3次握手模型，但实际上并不能保证对消息丢失进行了容错，只能在概率上降低消息丢失的可能。\n"});index.add({'id':5,'href':'/docs/safety-liveness/','title':"8. Safety 和 Liveness",'section':"Docs",'content':"8. Safety 和 Liveness #  Safety 和 Liveness是分布式系统中两个重要的属性。\nSafety #  一些不好的结果不会发生。\n如果不能满足safety，可以在有限的时间或者执行后被证明。比如之前提到FIFO，Causal, Totoal Order交付，如果违背了这些交付原则，那么经过几次执行后，我们就可以检测出来。\nLiveness #  一些好的结果最终会发生。\n如果不能满足liveness, 不可以在有限的时间或者执行后被证明。比如我们要设计一个系统对于所有的请求都要做回应，但是很有可能这个系统很慢，一个请求会在10年后进行回复，很显然，我们不能在有限的时间里去证明我们的系统违背了liveness.\n单独讨论Safety一个特性是没有意义的，单独讨论Liveness一个特性也是无意义的。我们会在后面讨论到CAP理论的时候再提及这两个特性。\n"});index.add({'id':6,'href':'/docs/chandylamport-snapshot-algorithm/','title':"7. Chandy-Lamport快照(Snapshot)算法",'section':"Docs",'content':"7. Chandy-Lamport快照(Snapshot)算法 #  分布式快照算法 #  在第一篇文章中我们谈到，如果系统某些节点出故障了，我们很有必要站在全局的角度去看系统的运行状态，这时我们就需要每个节点在这个时刻给系统拍个照片，记录下面每个节点的状态。这些照片的状态汇总在一起就可以得到全局状态。快照的主应用有故障恢复，死锁检测，垃圾回收等。\n上一篇文章末尾提到，“某一时刻“的表示不能用自然时钟，比如10:00中拍张快照，而是要用逻辑时钟。下面我们就介绍一个基于逻辑时钟的分布式快照算法Chandy-Lamport Snapshot。\nChandy-Lamport Snapshot #  Chandy-Lamport算法是分布式系统的中的各个节点对状态进行快照，而不需要一个集中的协调方(Coordinater)去协调。\nChandy-Lamport算法大致分为三个部分：快照发起，快照扩散，快照终止。\n快照发起 #  发起快照事件的进程：\n 首先对自己的状态进行录制 通过Outgoing Channel（出进程的网络通道）发送Marker消息给其他进程。 当接收到Incoming Channel（进入进程的网络通道）的消息时，开始对接收到的消息进行录制  上述过程简单来说就是记录下自己的状态同时记录下接收到状态。\n下图的例子中，波浪线的位置是发送Marker消息，在发送之前，P1中的状态已经被录制（图中圈出的部分），同时开始记录从P2发送过来的状态。我们记做C21，Cij代表的是i-\u0026gt;j的Channel的状态。\n快照扩散（Propagating） #  如果进程Pi是第一次见到一个从Cki Channel传递过来的marker，则\n Pi录制下自己当前的状态 Pi将Cki Channel标记为空 Pi发送marker给其他Outgoing Channel Cij Pi开始记录出了Cki的所有的消息。  如果Pi已经接收过一个marker\n Pi会停止录制其他进程过来的状态Cki, 这里用k表示其他的进程。同时按顺序将自从开始录制以来接收到的从进程k发送过来的消息记录到Cki中。  我们看一个例子。\n黄色的地方是P2第一次见到P1发送过来的marker, 所以P2先对自己的当前状态进行了录制(画圈的地方）。然后将C12标记成了空。\n然后P2将maker发送给P1，P1已经见过了这个maker（因为是P1发送的），所以在图中标绿的时刻，会停止录制，同时将自开始录制快照以来的所有接收的消息给记录到C21中。\n会不会发生下面的情况？\n我们要注意Chandy-Lamport算法一定是要在能保证FIFO交付的系统中才能正确运行，同时这个算法也是基于可靠网络传输的。\n我们再看一个更加复杂的例子：\nP1发起快照事件后，首先对自己的当前状态进行录制，同时将C21和C31设置为空。\nP1发送的marker先到到了P3。（为了防止波浪线看着太乱，下面的图我都用红色的线表示marker）\nP3接收到marker后，先对自己当前状态进行录制，设置C13, C23为空。同时发送marker到P1和P2。\nP2此时也是第一次见到这个marker，所以先对自己的当前状态进行录制，设置C12, C32为空，同时发送marker到P1， P3。\nP1不是第一次见到这个marker，于是停止录制，同时将自录制开始以来接收到的消息，H-\u0026gt;D，写入C21。\n剩下的过程我不继续画了，图已经很乱了。这是因为如果有N个进程，那么一次录制，则需要发送N * (N-1)条消息。\n快照终止 #  如果一个进程再次见到marker消息时，保存好已经记录的自己的状态和其他Channel的状态，快照算法结束。\nChandy-Lamport算法能保证算法一定会终止吗？答案是Yes，一个不严格的证明是，因为系统的传输是可靠的，所以每个进程都会见到同一条marker两次，然后结束快照。严格的证明感兴趣的同学可以阅读原论文。\nChandy-Lamport正确性的前提假设 #  Chandy-Lamport算法的正确性也是有前提的，我们前面其实已经提到过了：\n Channel是FIFO的 消息的传输是可靠的（消息不会丢失，或者被污染） 消息不会被重复错误发送 算法运行过程中进程不会崩溃。  上面的第4点也是Chandy-Lamport算法的一个局限性。\n去中心化(Decentralized) #  中心化的算法往往会有一个节点发起协调，比如有一个节点专门发命令给其他节点通知他们拍快照，很明显，Chandy-Lamport算法去中心化的，任何一个节点都可以发起快照命令。\n那如果有两个进程或者多个进程同时发起快照请求，算法还能保证正确性吗？答案是Yes，你可以通过下面的例子验证下。感兴趣的同学可以读原论文的严格证明。\n假设P1,P2在绿色的部分同时发起了快照请求，请你通过已经学到算法去验证下多个进程同时发起请求算法依旧是正确的。如果验证有问题，可以在评论区中指出。\n并不是所有的分布式算法都是去中心化的，我们后面还会学习到其他的中心化的和非中心的分布式算法。\n快照的用途 #  用途1：CheckingPoint，检查点。第一篇文章中我们提到在高性能计算的场景下，如果系统有partial failure，往往会将整个系统重启。系统重启后可以加载最新的正确的快照恢复到重启前的状态。\n用途2：死锁检测。当发生死锁时，拿到全局的快照更容易检测出死锁发生的地方和原因。\n用途3：更加通用的说，快照可以进行静止状态(Stable Property)的检测，死锁是Stable Property的一个特例。当发生死锁后，死锁的状态就不会发生改变，所以称作静止状态。另外的一个例子就是可以用于检测分布式任务是否完成， 如果所有的节点都完成了任务，那么状态就会变成已完成不会再改变。\n割线(Cut)\n一个割线(Cut)是指在Lamport diagram中，将其分成过去(Past)和将来(future)两部分。一个时间如果在一个割里面，那意味着这个事件在过去的部分。\n一致性割线(Consistent Cut)\n一致性割线是指对于在一个割中的所有的事件E，F，如果F-\u0026gt;E，那么F也在割中。\n上图中，左边的割线是不一致的，因为F-\u0026gt;E，但是F不在割中。右边的是一致性割。\n如果我们去验证一下Chandy-Lamport算法，会发现，所拍的快照是符合一致性割的。\n"});index.add({'id':7,'href':'/docs/consensus/','title':"12. 分布式共识算法",'section':"Docs",'content':"12. 分布式共识算法 #  首先思考一个问题，我们为什么需要共识算法，或者说为什么需要达到共识。\n如果我们有很多的进程，那么：\n 我们需要保证这些进程按照相同的顺序进行消息的交付。 我们需要记录跟踪这些进程中还在正在运行的列表。 我们需要其中的某一个进程完成一些特定的任务。 我们需要进程间轮流去互斥地访问共享的资源。 我们需要这些进程间对一个事务是否完成提交或者放弃达成一致。  上面的五个场景分别有专业的术语：\n 分布式原子广播问题(Distributed Atomic Broadcast Problem) 分布式失败检测(Distributed Failure Detection) 分布式领导选举(Distributed Leader Election) 分布式互斥问题(Distributed Exclusion Problem) 分布式事务(Distributed Transaction Commit Problem)  上面的这些问题你或多或少都应该了解过，那这些问题有什么共通点呢？\n这些问题都是在多个进程间需要互相协作去达成一个共识，尽管需要达成共识的具体问题会不一样。\n在分布式系统中，尤其是在崩溃错误模型和消息丢失错误模型中很难去做到达成共识，尽管我们也会学习一些分布式的共识算法，但是无论是理论难度还是工程难度都很高。\n共识 #  什么是共识(Consensus)呢？我们可以简单地抽象成下面的黑箱和输入输出，不同的输入在经过黑箱后产生相同的输出。\n上面的两个共识都是合法的，共识算法需要使得所有的进程都会为共识的达成作出贡献同时最终所有进程都会同意某一个值。所以上面的两个例子都是合法的。\n看上面的第一个例子你可能觉得共识很简单，比如都取输入的众数(大于一半)作为共识，但是别忘了我们是站在一个协调者的视角来看的，但是在分布式系统中，是需要进程间在可能存在失败的异步网络中相互协调去达成共识的。\n共识算法 #  在介绍具体的共识算法前，我们先了解下共识算法的特性。\n共识算法会尝试去满足下面的三个特性：\n 终止(Termination): 所有的正确的进程最终会决定好一个值。 共识 (Agreement): 所有的正确的进程最终会在一个相同的值上面达成共识。  假设我们的算法只满足了上面两个特性，可行么？\n答案是 NO。比如下面的例子\n所以，三点特性是：\n 终止(Termination): 所有的正确的进程最终会决定好一个值。 共识 (Agreement): 所有的正确的进程最终会在一个相同的值上面达成共识。 有效性(Validity): 达成共识的值必须是之前提议过的。  共识算法需要满足上面的三条特性，但是目前为止还没有算法能够满足所有的三点，至少在异步网络模型中。为什么？\n一句话说就是不可能。有一个理论叫做FLP不可能定理(FLP Impossibility)，在异步网络模型中，只要有一个进程失败了，那么整个系统都不能达成一致。具体的证明是有严格的数学推理，感兴趣的同学自行查询原论文呢阅读。\n如果我们做些妥协呢？其实我们可以妥协下第一个特性：所有的正确的进程最终会决定好一个值。那就意味着可能一点时间内不是所有的进程都做好的决定，那我们可以再来一次，看是不是能达成共识。\n目前比较著名的共识算法有Paxos和Raft，会分别用两篇文章加以介绍。\n"});index.add({'id':8,'href':'/docs/message-delivery/','title':"6. 消息交付(Message Delivery)",'section':"Docs",'content':"6. 消息交付(Message Delivery) #  消息交付(Message Delivery)\n消息交付和消息的接收(Receiving)，发送(Sending)在于\n 我们可以控制消息的发送，比如控制发送的频率 消息的接收对于我们来说是一个事件，一条消息已经传送到了我们的网络设备上 消息的交付也是我们可以控制的，比如消息经由网络传输到我们进程后，我们可以决定先放队列里不处理。  消息的交付是由应用程序决定的，有时消息接收后不会立即处理，会被放在队列里面，想想看为什么会这么做呢？其中一个原因可能是服务繁忙，消息太多不能及时处理，所以放到队列里。另外还有原因可能是消息接收后是无序的，所以需要放队列里等无序的消息到齐了再有序地交付。\n那会有哪些有序消息交付呢？\nFIFO #  FIFO就是First in, first out。相信这个原则大家都不陌生。但是这里还是给出严格的定义\n如果一个进程先发送的消息m1, 后发送的m2，那么，所有的消息接收进程都应该先交付m1,后交付m2。\n我们看下下面的这个例子，图中表示的分别是P1进程的消息发送事件和P2的消息交付事件。\n显然, P2进程应该先交付m1,后交付m2，这里就违背了FIFO。(FIFO violation)\n我们可以认为m1和m2塞到了一根管道里面，出来的时候应该是m1后m2。\n从上面的FIFO violation可以看出设计一个队列将消息缓存起来保证消息的有序交付还是很有必要的。\n对于分布式系统来说，一般也没必要去设计一个FIFO的消息交付，比如我们可以使用TCP进行传输，TCP是可以保证消息的有序性的。\n保证因果交付(Causal Delivery) #  前面数篇文章都在谈论因果，因果是是指事件发生的先后顺序。对于消息发送事件，如果m1的发送happens before m2，那么m1的交付事件也应该happens before m2。\n我们再看下这个例子：\n很明显，m1 happens before m2，但是交付的时候m2 happens before m1，这是违背了Causal Delivery原则的。\n上面的例子其实就是第一个违背FIFO Delivery原则的例子，我们可以看出如果违背了FIFO Delivery，也必然违背了Causal Delivery。这是因为，根据happens before的定义，如果同一个进程里m1在m2之前发生，则m1 happens before m2。这正符合FIFO中发送消息的要求。\n违背了FIFO Delivery必然违背Causal Delivery，那反过来呢？如果违背了Causal Delivery，是否一定违背了FIFO Delivery呢？\n我们看一个之前章节看过的例子：\n根据我们之前学习，Bob发送“讨厌你“给Carol是发生在Alice发送“Bob很蠢\u0026quot;之后的，但是Carol进程的消息交付却是先交付了“讨厌你“，很明显，上面的例子违背了Causal Delivery。\n上面的例子违背了FIFO delivery么？\n答案是没有违背。 有同学可能会问，Bob发送“讨厌你“事件 和 Alice发送“Bob很蠢\u0026quot; 事件，消息交付顺序错了。这是违背了Causal Delivery。我们回头看FIFO Delivery，其要求是“同一进程“内的消息交付有序性。\n所有，我们设计了一个分布式系统，如果能够保证FIFO delivery，其实是不能保证Causal Delivery的。\n保证完全有序交付(Totally Ordered Delivery) #  我们上面学习了保证同一进程内消息有序交付的FIFO Delivery以及保证事件因果交付的Causal Delivery，下面我们看另一种交付，Totally Ordered Delivery，简写TO。\n如果一个进程先交付了m1,然后m2，那么所有的进程都应该先交付m1然后m2。\n同样的，我们举个违背TO的例子。\n起初两个备份节点R1, R2的数据是一致的，X都等于0，然后两个客户端分别发送了消息更新X为1，更新X为2，但是R1的消息交付顺序是m1, m2，而R2消息交付顺序为m2, m1。这是违背了所有进程消息交付顺序一致的原则的。\n一个事件发出多条消息，这个事件叫做广播(BroadCasting)，后面会讨论到。\n对于上面的两个广播事件的消息投递，我们已经知道是违背了TO的，那么他们有违背FIFO或者Causal Delivery吗？\n分层 #  一个系统如果不能保证FIFO，也必然不能保证Casual Delivery，一个系统如果能Casual Delivery则一定能保证FIFO。\nTotally Ordered Delivery应该放哪里呢？放在Casual Delivery上面么？如果你想清楚了上面Casual Delivery的思考题，那么这里应该回答NO。\n实现了TO其实也不能保证FIFO或者Causal Delivery。\n实现FIFO Delivery #  尽管前面提到我们很少需要去实现FIFO Delivery，但这里还是做一下简短的介绍。\nFIFO的定义是：如果一个进程先发送的消息m1, 后发送的m2，那么，所有的消息接收进程都应该先交付m1,后交付m2。\n或者等价于说同一个消息发送方发送的消息交付要跟发送顺序一样。\n一般来讲，有两种实现方式。\n序列号(Sequence Numbers) #   发送方对每条加上序列号和进程id 发送方每次对序列号+1 接收方如果发现接收的消息序列号不是上次消息序列号+1，则放入队列等待到正确的消息到达。  这种方式很好理解，但是也会存在一些问题。\n如果消息传输丢了怎么办？\n有一种策略就是消息丢了的话继续交付，不进入队列，但是无法很好的处理消息的延迟问题：\n消息确认(ACK) #  发送方和接收方通过发送，确认，发送，确认的方式进行，这种方式的缺陷是效率很低。当然也有一些解决方法，感兴趣的同学可以自行查阅计算机网络介绍TCP的知识。\n实现因果广播(Casual Broadcast) #  算法 #  我们再来看下违背Casual Delivery的例子\nBob发送“讨厌你“给Carol是发生在Alice发送“Bob很蠢\u0026quot;之后的，但是Carol进程的消息交付却是先交付了“讨厌你“。\n为了实现causal delivery，我们可以借助之前学习的向量时钟(Vector Clock)来跟踪消息以确保消息保证因果关系交付，但是会对向量时钟做一些修改：时钟只记录消息的发送，不记录消息的接收。\n下面是算法详情：\n 消息在发送前，先将本地时钟+1，然后将本地时钟随消息一起发送。 如果进程P1发送的消息在P2交付了，则将P2时钟P1所在的位置+1。 P2仅仅在满足了下面条件时才会交付P1的消息：  P2上P1位置的时钟 + 1 = 接收到的P1时钟 P2上除P1位置的时钟都应该\u0026gt;= P1上除P1以外对应其他位置的时钟。    我们验证下上面的算法。\n这个算法有三点要注意，首先，消息接收事件不会对时钟进行计算，其次，只有在消息交付后才会对时钟进行+1。\n图中标绿的地方，Carol中Bob的时钟是0，Bob传输过来的Bob位置时钟是1，满足条件3.1，但是，Carol中Alice的时钟小于Bob传输过来的的Alice的时钟，这里不符合条件3.2。Carol中Alice的时钟小于Bob传输过来的的Alice的时钟意味着Alice有发送消息，但是Carol还未进行交付。此时，绿色部分的消息会被进行缓存。\n图中黄色的地方，Carol中Alice的时钟是0，Alice传输过来的Alice位置时钟是1，满足条件3.1，Carol中其他地方的时钟都大于等于Alice其他地方对应的时钟，满足3.2。此时，黄色部分的消息会被交付。\n黄色部分的消息被交付后，绿色的消息也就符合条件，进行交付。\n下面的例子留给你练习下这个算法。\n因果关系的应用 #  因果关系，或者说happens-before关系，我们学习了可以用来判断事件发生的先后顺序进行系统的debug，也学习了用来保证因果交付，下一篇文章我们会介绍因果关系的另一个应用：快照算法。快照是在某一个时刻将整个系统的所有状态进行保存，就好像给系统拍个照片一样。如果我们设定为每天上午10点给系统做快照，有可能因为各个机器间时间不一致，导致有些状态并没有被加进快照，如下图，我们拿到M1和M2的两个快照，发现M2有接收到M1的消息（红圈标记的），但是M1的快照中并没有记录这条消息的发送。这时，自然时钟是不适合的。\n"});index.add({'id':9,'href':'/docs/raft/','title':"分布式共识算法-Raft",'section':"Docs",'content':"分布式共识算法-Raft #  S\n"});index.add({'id':10,'href':'/docs/protocol-lamport-diagram/','title':"5. 网络协议(Protocol)与Lamport diagram",'section':"Docs",'content':"网络协议(Protocol)与Lamport diagram #  网络协议简单来说就是制定的一些发送接收方遵循的规则。\n比如我们制定一个这样的网络协议：\n 一方发送\u0026quot;How are you“ 一方则回复\u0026quot;Good!\u0026quot;  下方是这样协议的一个示例\n那我换一种协议运行(Protocol Run)方式呢？是协议异常(Protocol Anomaly)吗？\n很显然，也是正确的，因为协议中并没有规定只能Alice先说\u0026quot;how are you\u0026quot;。\n那下面的呢？\n显然是错误的，协议规定必须是先\u0026quot;how are you\u0026quot;，然后回复\u0026quot;Good\u0026quot;。\n接下来的这个呢？\n这个例子有点奇怪，但是并没有违背协议。有可能Bob喝醉酒了，回复的有点慢。像下面这样子。\n从上面的几个例子，我们可以得出结论，Lamport diagram不能描述事件发生的时间长短，但是可以描述事件的先后顺序以及违背协议的情形。\n上面我们猜测Bob可能喝醉酒了才回复的慢，机器当然不可能喝醉酒。一个合理的猜测就是处理的慢，Lamport diagram中的点只能表示消息接收了(Message received)，但是不能表示消息的投递(Delivered)的完成。两者的区别我们下次讨论。\n上面我们猜测Bob可能喝醉酒了才回复的慢，机器当然不可能喝醉酒。一个合理的猜测就是处理的慢，Lamport diagram中的点只能表示消息送达(Message sent)，但是不能表示消息的交付(Delivered)的时间。两者的区别我们下次讨论。\n"});index.add({'id':11,'href':'/docs/vector-clock/','title':"4. 向量时钟(Vector Clock)",'section':"Docs",'content':"4. 向量时钟(Vector Clock) #  上一篇文章中我们知道，如果事件A happened before B，那么Lamport时钟LC(A) \u0026lt; LC(B)，但是如果LC(A) \u0026lt; LC(B)，我们并不能推断出A happens before B。\n这里介绍另外一种时钟，向量时钟(Vector Clock)。这个算法是Friedemann Mattern and Colin Fidge在1988年发表的。\n向量时钟特点是，如果事件A happened before B，那么A的向量时钟，记做VC(A) \u0026lt; VC(B)，并且，如果VC(A) \u0026lt; VC(B)，可以推断出事件A happened before B.\n通过对比Lamport时钟，我们发现Lamport时钟仅仅是遵循因果关系，而向量时钟可以描述因果关系。\n所谓的因果关系，并非逻辑上的因果关系，而是事件的发生顺序。\nLamport时钟是用一个数字来表示，那向量时钟呢？从名字可以看出来是用一个向量，是一组描述事件顺序的数字。\nStep 1，每个进程初始化一个向量。每个向量的长度跟进程数相等。比如如果有A B C三个进程，则每个进程都初始化为[0, 0, 0]。\n如果我们不知道进程数呢？这的确是向量时钟算法的一个缺陷，也导致了向量时钟的运用范围会窄一些。\n如果某一时刻某个向量是[17, 0, 0]，含义是， A进程中发生了17次事件， B， C都为0。回顾下，Lamport时钟里是全局的计数器，而向量时钟是进程的局部计数器。\nStep 2，每个进程中，内部事件，消息发送，接收事件发生时，都将计数器+1。\n如果A进程中又发生了一次事件，那么新的向量为[18, 0, 0]。这个和Lamport很相似。但关键是消息接收事件的处理，Lamport算法会将本地的计数器和网络携带过来的计数器取最大值，向量时钟该如何取最大值呢？\nStep 3，如果是消息发送事件，计数器+1后将向量时钟携带着一起发送\nStep 4， 如果是消息接收事件，则对两个向量相同位置取最大值。这个比较方法叫做point wise。比如[1, 12, 4]，和[7, 0, 2]，其最大值向量就是[7, 12, 4]。\n在对上述算法进行演示前我们先思考一个问题，也就是VC(A) \u0026lt; VC(B)，小于号这个操作符该如何定义？可能你已经猜到了，我们需要对两个向量里相同位置的值进行比较。\nVC(A) \u0026lt; VC(B)当且仅当\n VC(A)i \u0026lt;= VC(B)i，对于所有的i满足 VC(A) 不等于 VC(B)  下面的三个例子，哪些可以满足VC(A) \u0026lt; VC(B)？\n VC(A) = [2, 2, 0]，VC(B) = [3, 2, 0] VC(A) = [2, 2, 0]，VC(B) = [1, 2, 3] VC(A) = [2, 2, 0]，VC(B) = [2, 2, 0]  答案是仅仅第1个。第2个例子不满足条件1， 第3个例子不满足条件2。\n对于第二个例子，VC(A) = [2, 2, 0]，VC(B) = [1, 2, 3]，我们不能推断出VC(A) \u0026lt; VC(B) 或者VC(B) \u0026lt; VC(A)，这种状态我们称为他们是独立的或者并发的。我们之前提到过，A和B是不能推断出的A先于B发生或者B先于QA生， 那我们称这种关系叫做并发(Concurrent)。记作A || B。这种关系也叫做Q和R是独立的(Independent)，无依赖的。\nOK，我们开始来用一个例子演练下向量时钟算法。\n我已经计算出了部分答案，你可以自己动手也算一下。\n答案如下\n我们之前提过，事件之间如果存在确定的先后关系，那么在视觉上可以用线给连起起来，验证工作就交给你完成了。\n我们看下下图中事件A和B，显然，他们是独立的，我们无法确定他们的先后关系，无论是数值上还是视觉上。\n向量时钟就先介绍到这里，后面会多次和向量时钟打交道。\n"});index.add({'id':12,'href':'/docs/lamport-clocks/','title':"3. Lamport时钟(Lamport clocks)",'section':"Docs",'content':"3. Lamport时钟(Lamport clocks) #  我们之前谈到，我们需要一种仅仅用来标注时间的发生顺序的逻辑时钟，这种时钟不会告诉我们现在是几点钟了，也不会告诉我们过去了多长时间。 Lambert clock。\nLambert clock其实是一种给事件分配数字的方式，使得如果事件A早于事件B发生，那么A的 Lambert clock，简称LC，小于事件B的LC。LC(A) \u0026lt; LC(B)。\n所以我们说 Lambert clock是和因果关系保持一致的，如果事件A早于事件B，则LC(A) \u0026lt; LC(B)。\n下面是算法的具体过程\n 首先，每个进程都初始化计数器(Counter)为0。 每个进程内，事件发生则将计数器+1。 如果发生消息发送事件，则将其计数器携带上。 如果是消息接收事件，则max(本地计数器+1, 网络携带的计数器)。  其实很简单的过程，你可以试着计算下下面的例子\n答案\n那思考一个问题，如果LC(A) \u0026lt; LC(B)，那么可以推断出事件A happens before B么？\n答案是否定的。\n我们来看另一个例子。你可以再试着计算下下面的例子中事件A和事件B的LC分别是多少。\nOK，相信你已经计算好了。\n我们发现，LC(A) = 1, LC(B) = 5，LC(A) \u0026lt; LC(B)，那A事件 happens before B事件么？NO。\n根据之前讲过的Happens-before的定义：\n A和B在同一个进程线里面而且B在A后面发生。 A是消息发送事件， B是与之对应的接收事件。 如果A-\u0026gt;C, 且C-\u0026gt;B。  显然，这里的A事件和B事件之间不符合上面三条定义中的任意一个。也就是说，A和B之间是无关联的。所以如果知道LC(A) \u0026lt; LC(B)并不能推断出A happens before B。\n我们可以发现，如果两个事件之间存在一个确定的Happens-before关系，在图中是可以画一条线连接两个事件的。如果不存在在一个确定的Happens-before关系，则不能画出这样的一条线。\n当然，上图中我没有单独将各自进程内的事件连线。\n上面的例子摘自论文Detecting Causal Relationships in Distributed Computations: In Search of the Holy Grail，这篇论文发表在1994年，作者Mattern也发明了另一个逻辑时钟，一个能描述因果关系的时钟，Vector Clock，这个会在后面介绍。\n说回来Lamport Clock， 既然它并不能描述因果关系(Causality)，那我们为什么还要用它呢？\n先说过数学问题，如果P能推出Q, P=\u0026gt;Q，那么逆否命题～Q=\u0026gt;～P成立。\n如果A happens before B, 也就是 A -\u0026gt; B，则 LC(A) \u0026lt; LC(B)。 那么如果LC(A) \u0026gt; LC(B)，那么A -\u0026gt; B肯定为假，也即是A肯定不会happens before B。\n逻辑很简单，那么有什么用呢？Debug。\n下一节我们讨论vector clocks。\n"});index.add({'id':13,'href':'/docs/lamport-diagrams-happensbefore-partial-order/','title':"2. Lamport Diagrams，Happens-before，同步网络与异步网络，偏序关系",'section':"Docs",'content':"2. Lamport Diagrams，Happens-before，同步网络与异步网络，偏序关系 #  Lamport Diagrams #  在上一小节中谈到了逻辑时钟来推理事件之间的关系，这一小节介绍Lamport Diagrams。\n这个图很简单，一眼就能看懂。图中的点表示事件。\n从上图得出以下三个推论，哪些是正确的，哪些是错误的？\n X可能导致了Z Z可能导致了X X一定导致了Z  很显然，推论1是正确的，而2 3都是错的。\n我们再来研究下多台机器的情形。\n我们有m0, m1, m2三台机器，注意，这三台机器间没有共享内存(Shared Memory)，它们之间唯一的通信方式就是发送和接收消息。\n在m1中，事件X, Y, Z都是内部事件(Internal Events)，而P, S ,T, U, Q, R则是消息发送或者接收事件。\nHappens-before #  给定两个事件A,B，我们说A在B之前发生， A happens before B, 可记作: A -\u0026gt; B, 只需要满足以下的条件之一即可：\n A和B在同一个进程线里面而且B在A后面发生。 A是消息发送事件， B是与之对应的接收事件。 如果A-\u0026gt;C, 且C-\u0026gt;B。  上述的是解释了Happens-before, 但并不是具体的算法，具体的算法会在后面章节提到。\n同步网络和异步网络 #  比如alice，bob和carol在各自的进程中给其他人发送消息，如下图：\nAlice给BOB和Carol同时发送了\u0026quot;Bob很蠢\u0026quot;，BOB回应说“讨厌\u0026quot;，结果Carol先接收到了\u0026quot;讨厌\u0026quot;。这个问题我们在上节做过描述，有可能是网络延时导致的。Carol此时很茫然，这种现象叫做causal anomaly。\n这里我更想介绍的是两种网络模型。\n第一个是同步网络(Synchronous Network Model)。如果我们知道Alice发送了消息后会多久到达Bob， Carol那里，那这样的bug就可以避免了。如果一个网络里面的消息都会通过一个固定的时间N送达(\u0026lt;=N)，那这样的网络叫做同步网络。现实生活中，我们几乎使用的其实是异步网络。但是同步网络还是具有很高的科研价值。\n异步网络（Asynchronous Network Model), 与同步网络相反，并不存在一个N保证N时间内消息的送达。我们后面讨论的都是异步网络模型。\n状态和事件（state and events） #  我们之前提到，Lamport Diagram中的一个点就是一个事件，\nM1和M2之间的消息：M1询问M2 X是多少， M2回复x = 5。这些的确是一些事件，但同时也是状态。\n回忆下在计算机体系结构课程中，状态是指某一时刻的register之类的东西，这是指机器的状态。现实生活中，某一个时刻物体的属性也是状态。所以在Lamport Diagram图中的点既是事件，也是状态。\nM2此刻知道X是5，那之前一定有一些事件将X变成了5： 可能是一个事件直接把X设为5，也有可能有个事件将X设为1，然后又有4个事件把X递增到了5。一个状态其实是由一个或者多个事件导致的。\n所以， 如果我们回去把所有的事件给列出来，那我们可以计算出最终的状态。反过来可以么？我们知道了当前状态，能推算出之前的事件么？显然不行。\nHappens-before 回顾 #  上面我们提到，A Happens before B (A -\u0026gt; B)， 当：\n A和B在同一个进程线里面而且B在A后面发生。 A是消息发送事件， B是与之对应的接收事件。 如果A-\u0026gt;C, 且C-\u0026gt;B。  如果我们有两台机器，M1, M2:\n我们从上图可以推断,\n X-\u0026gt;Y因为X是消息发送事件， Y是与之对应的接收事件。 Y-\u0026gt;Z, X-\u0026gt;Q 因为在各自进程线里他们之间有先后关系 因为X-\u0026gt;Y, Y-\u0026gt;Z，所以X -\u0026gt; Z.  上面的三种关系叫做最小关系 （Smallest relation)。\n这些最小关系的表示可以有: (A, B), (B, C), (A, C), (D, C)。你能解释下这四个最小关系分别用的哪个条件推断出的吗？\n(A， D)对么？\n(A, D)并不能通过最小关系推断出，尽管图中看起来可能A -\u0026gt; D，\n再看下上面的一个例子：\n以下的几个时间的先后关系可以通过happens-before条件推断出吗？\nX和Z， P和Z， Q和R？\nQ和R是不能推断出的Q先于R发生或者R先于Q发生， 那我们称这种关系叫做并发(Concurrent)。记作Q || R。这种关系也叫做Q和R是独立的(Independent)，无依赖的。\n偏序(Partial Order) #  设R是集合A上的一个关系，如果R是\n 自反的 (Reflexivity) 反对称的 (Anti Symmetry) 可传递的 (Transitivity)  则称R是集合A的偏序关系，简称偏序，记作“≤”。\n对于Happens-before关系来说，很明显可以满足反对称性和传递性。但是不能说A比A发生的早，所以不存在自反性。但是我们可以说Happens-before是偏序的。\n"});index.add({'id':14,'href':'/docs/distributed-systems-what-and-why-time-and-clocks/','title':"1. 什么是和为什么分布式系统？时间和时钟。",'section':"Docs",'content':"1. 什么是和为什么分布式系统？时间和时钟。 #  什么是分布式系统 #   一个系统通过网络连接运行在多个的节点上 分布式系统的主要特点之一是局部失败(Partial Failure)  局部失败 #  运行在多个节点上的定义应该很好理解，第2点的局部失败又是什么呢？\n对于单个节点，或者说单台机器，如果失败了，一般重启也就能解决问题。但是对于多个节点构成的集群系统，如果集群中的个别机器失败了，很明显，将集群重启并不是很理想的做法。对于分布式系统而言，尽管存在一些失败的节点，整个系统需要保证依旧正常运行。而局部失败是不可避免的，比如网络故障，硬盘机械故障等等，所以说分布式系统的主要特点之一是局部失败。\n高性能计算(HPC, High performance computing) #  相较于分布式系统，HPC往往将局部失败当作整体失败(total failure)对待，而分布式系统在局部失败时需要正常运转。\n分布式系统会有些Failure #  假设我的系统由两台机器M1, M2组成，M2中存储了X的值，如果M1想知道X的值，便会向M2发起请求，那么会有哪些情形发生呢？\n可能的情形有：\n 从 M1发送的消息在传输过程中丢失。 M1的消息发送后很长时间才被M2接收。比如网络延时，或者 M2有消息队列积压较多。 M2宕机了。  那M1将消息发出后，它能感知到上面的情形么？实际上还有些情形比如\n M2回复的很慢 M2的回复在回传过程中丢失了  很明显，M1在发出消息后如果没有接收到回应，M1自身是不可能感知到是出了什么问题的，除非有一个掌控全局的观察者告诉M1发生了什么。\n还存在其他的问题么？\n M2可能撒谎，X明明是3，却说X等于5 M2可能拒绝回答。 可能有宇宙射线，传送过程中将X的值置成了错误的值。 \u0026hellip;  上述的这些问题叫做拜占庭故障(Byzantine faults)， 而且都是在真正的分布式系统中可能发生的。比如分布式的货币系统（比特币），每个用户系统都记录了其他用户的余额，如果有些用户的系统被黑客攻击了，对金额撒谎怎么办？\n网络超时 #  M1此时发送消息给M2要求把X的值+1，M2在完成任务后发送 “OK“确认。\n如果M1没有接收到M2的OK回应，那么M1怎么才能确认\u0008X是否被加过1了呢？X+1的请求可能没有传输到M2，也有可能M2处理的太慢，很久才回复，也有可能M2处理了，但是OK消息在回传M1时遇到网络故障传送失败了。在很多时候，我们会对网络请求进行超时(Time out)处理，那M1的请求超时后可能会重新发送X+1的请求，很明显，这样会容易导致X被执行了两次+1。超时机制并不能帮我们识别失败发生在哪里了。这些不确定性也正是分布式系统不可避免的。\n我们可以通过统计手段，找出一个分布式系统内正常情况下最大的超时时间：比如1s内正常的网络请求都能够完成，这样当故障发生时可以帮我们排除一些失败，但是并不能帮我们排除所有的失败的可能。所以一个系统发生了延时，也会给我们带来无尽的麻烦。\n分布式系统 = 局部失败 + 无尽的延时。\n分布式系统这么复杂，为什么还要使用它？ #  的确如此，分布式系统无论是开发起来，还是debug都很复杂，但是\n 数据可能太大了，一台机器放不下，我们不得不分布式 可能单台机器算的太慢了，我们想加快运算  时间和时钟 #  计算的时候，时钟被用来干嘛了？\n 用于标定时间。比如，晚上9:00去机场，浏览器缓存到4月1号0点过期。 时间段或者间隔。比如去机场要45分钟，浏览器缓存在1天后过期。  其实上面的表述中，“晚上9:00去机场，浏览器缓存到4月1号0点过期。“ 用的都是自然天时钟(time-of-day clock)，我们也知道可以用网络时间协议(NTP,The Network Time Protocol)进行服务器之间的时间同步。然而，自然天时钟用于标定时间很好，但是不适合用于表示时间段或者间隔。\n单调时钟(Monotonic clock)。单调时钟只往前走不往后，比如系统开机后经过了多长时间就是个单调时钟， UNIX时间戳(Time Stamp)也是单调时钟。单调时钟很适合用于表示时间段或者间隔，但是不适合标定时间。\n关于非单调时钟的问题，感兴趣的同学可以阅读CloudFare因为非单调时钟导致的一次故障：https://blog.cloudflare.com/how-and-why-the-leap-second-affected-cloudflare-dns/\n上述的两种时钟都是物理时钟，在分布式系统中，我们需要一种新的时钟，逻辑时钟。\n逻辑时钟仅仅用来标注事件的发生顺序，最基本的讨论点就是哪个事件先发生的。\n比如说，M1机器需要从M2读X， M3需要修改M2的X，那我们就很有必要搞清楚哪个事件先发生的。\n来思考一个逻辑问题， 如果A早于B发生，那我们可以推出什么样的因果关系？\n A 可能导致了B B不可能导致A  这种基于时序的推理很有利于系统的开发和debug。 如果A先于B发生，我们开发的系统就不能让用户知道是B在A之前发生。\n"});index.add({'id':15,'href':'/docs/paxos/','title':"分布式共识算法-Paxos",'section':"Docs",'content':"分布式共识算法-Paxos #  Paxos是最著名的分布式共识算法之一。这里介绍一下这个算法。\nPaxos的角色 #   Proposer，提议者 - 发起提议 Acceptor，接受者，对于Proposer提议的Value作出决定。 Learner，学习者，学习已经达成一直的Value。  当我们使用Lamport Diagram来表现Paxos的时候，为了理解方便，我们需要将上面的三种角色分开来用不同的进程表示，但是实际运行中一个进程可以扮演多个角色。\n一个问题是Paxos的节点是需要提前知道有多少个Accpetor的，因为要达成一致，需要知道有大于等于一半的Acppetor同意。\n在介绍具体算法前，这里先澄清下，我们接下来先讨论的是对单个值达成共识，而对于一系列的值达成共识会在更后面进行介绍。\n算法运行 #  假设某一时刻一个Proposer, P1，想要提议一个值，首先它需要做的是发送一条Prepare消息到至少众数的Accpetor那里。\n发送的Prepare消息的同时需要携带上唯一的提议号，Proposal Number。这个Proposal Number需要时全局唯一的，也就是说一旦一个Proposer使用过了这个提议号，那么其他的Proposal就不能再使用。所以如果有两个Proposer，一个简单的方式就是分别使用奇偶不同的Proposal Number。\nProposal Number的另一个要求是必须是单调增的，也就说新的Proposal Number必须比已经使用过的Proposal Number来的大。\n当一个Acceptor接受到了一条Prepare消息后，它需要看一下这个proposal number：\n 如果之前没有对这个proposal number作出过承诺，那么就回复给Proposer一条Promise消息。 如果之前已经承诺过这个proposal number，那么直接丢弃这条Prepare消息。  上面的例子中Proposer P1给AcceptorA1, A2发送了Prepare 5的消息，这里其实也可以发送给A3，但是为了简化，只发送给了A1， A2，但是也达到了众数，所以是正确的，现实中也可能是消息丢失了最终只有A1和A2接收到了这条Prepare消息。\nA1和A2都没有对5作出过承诺，所以会回复Promise 5的消息。\n这里我们就已经达成了第一个里程碑： 众数个的Acceptor承诺了一个proposal number。如果此时还有其他Proposer发送prepare 4或者prepare 5，都不会被Acceptor承诺。\nProposer P1此时接收到了众数个的Promise消息，于是它可以发送给accept消息给众数个的Acceptor。Accept消息包含proposal number和实际上需要进行提议的数值。\nP1提议的值是1所以accept消息包含proposal number 5和要提议的值1。而Acceptor在接收到accept消息，询问自己是否对这个proposal number作出个承诺，\n 如果YES，则接受这条accept消息，回复一条accepted消息给Proposer和所有的Learners。 如果NO，忽略这条accpet消息。  众数的Acceptor发送了accepted消息后的这个时刻，系统就达成共识。🎉。\n上面就是一个简单的Paxos算法运行的过程。\n达成共识的这个时刻，我们会发现其实系统中的每个节点并不是全部意识到了共识已经达成。最明显的就是A3，它什么都没收到！A1和A2发送了Accepted消息后，指导P1接收到了这两条消息才知道共识已经达成，而A1和A2此时还是浑然不知共识是否达成，因为他们不知道其他节点发送了什么消息。\n所以共识的达成和节点知道共识达成是两回事。\nP1只需要将消息发送给众数个节点的acceptor就能保证算法的正确性，但是如果P1能将消息发送给所有的acceptor的话会更好，因为存在消息丢失的可能性，所以消息发送给越多的acceptor，更容易使得众数个节点接收到消息。\n类似的，当P1从其acceptor接收到Promise消息时，它需要确保接收到了来自众数个节点的消息。当然，接收到全部的节点的消息更棒了！\n我们再来回顾下上面的算法，在系统达成一致的过程中，有哪些里程碑（milestones）的事件？用M表示。\n不难发现，第一个里程碑的达成是当众数个acceptor对一个提议号作出承诺的时候。\n第二个里程碑是共识达成的时候，也就是当众数个acceptor发送了对一个提议号accepted消息的时候。尽管此时各个节点并不知道共识已经达成。\n第三个里程碑是当Proposer或者Leaner从众数个acceptor接收到accepted消息的时候，也是这个时候他们知道系统已经达成了共识。\n上面就是简单版的Paxos运行过程，下面我们把Paxos算法的步骤详细地描述一下。\n算法流程 #  我们从上面的简单版本Paxos的运行过程，可以得出一个初步的算法流程：\n1，Proposer\n发送Prepare(n)消息给众数个acceptors，n必须满足\n 唯一性 大于其他任何已经尝试过的提议号（Proposal Number）  2，Acceptor\n当Acceptor接收到一条Prepare(n)消息时，会根据是否对提议号n作出过承诺而作出不同的反应：\n 如果已经承诺过了，丢弃这条消息 如果没有承诺，则回复Proposer一条Promise(n)的承诺消息(*)，消息的含义是我会承诺丢弃掉所有小于n的消息。  3， Proposer\n当Proposer接收到来自众数节点的对提议号n的Promise消息后，发送Accept(n, val)给Acceptor，n是提议号，val是具体的数值(*)。\n4，Acceptor\n当一个Acceptor接收到Accept(n, val)小时的时候，会询问自己是否承诺过丢弃n?\n 如果Yes，则丢弃这条消息 如果NO，则回复Accepted(n, val)消息给Proposer，同时将Accepted(n, val)发送给所有的Learner节点。  OK，我们开始研究下如果有多个Proposer节点的话，上面的算法需要做怎么样的修改才能保证正确性。为了使流程图看起来不那么乱，下面的例子会省略去Learner节点。\nP1先发起了共识请求，我们再来看下，如果P2此时发送了prepare (4)消息给两个acceptor，会是什么结果\n此刻，A1和A2都已经承诺过丢弃提议号小于5的消息，P2此时发送给A1和A2的消息自然也就会被丢弃，A3还未作出过承诺，所以A3此时可以承诺会丢弃小于4的消息。\n显然，此时的P2是不可能收到众数节点对提议号4的承诺的，那它应该怎么办呢？\nP2在等待超时后，可以重新发起Prepare(n)，那它该发送怎么样的n呢？显然P2可以发送一个比4更大的数，比如5，结果还是超时，再尝试6，可能可以得到众数个的Promise消息。当然，我们也可以预先设置好规则，P1使用奇数的提议号，P2使用偶数的提议号。\n假设P2超时了，然后决定发送Prepare(6)给所有的acceptors，此时，3个proposer都没有承诺过丢弃6，按照之前的算法定义，应该是所有acceptor回复Promise(6)消息。\n 2，Acceptor\n当Acceptor接收到一条Prepare(n)消息时，会根据是否对提议号n作出过承诺而作出不同的反应：\n 如果已经承诺过了，丢弃这条消息 如果没有承诺，则回复Proposer一条Promise(n)的承诺消息(*)，消息的含义是我会承诺丢弃掉所有小于n的消息。   我们对上面*号的地方略作修改：\n 2，Acceptor\n当Acceptor接收到一条Prepare(n)消息时，会根据是否对提议号n作出过承诺而作出不同的反应：\n 如果已经承诺过了，丢弃这条消息 如果没有承诺，则询问我之前作出过其他承诺了吗？  如果Yes，则回复Proposer一条Promise(n, (之前承诺的n, 之前接受的val)) 如果NO，则回复Proposer一条Promise(n)的承诺消息，消息的含义是我会承诺丢弃掉所有小于n的消息。     A1和A2因为都承诺过提议号5和接受过val1，所以他们会回复Promise(6,(5,1))，而A3因为还没有作出过任何承诺，回复Promise(6)。\n那P2在接收到上面的Promise消息后该怎么做呢？相较于Promise(6)这样的消息，Promise(6,(5,1))则包含了Acceptor之前承诺的最大提议号。\n我们再回顾下Proposer的此时的算法流程。\n 3， Proposer\n当Proposer接收到来自众数节点的对提议号n的Promise消息后，发送Accept(n, val)给Acceptor，n是提议号，val是具体的数值(*)。\n 这里P2不能随意地发送 Accept消息了，而是需要从promise消息里面取出具有最大的提议号的接受的val，将其发送给Acceptor。\n所以此处算法修改为：\n 3， Proposer\n当Proposer接收到来自众数节点的对提议号n的Promise消息后，发送Accept(n, val)给Acceptor，n是提议号，val是:\n 如果Proposer接收到任何的Promise(n, (prevN, prevVal))，它必须从中选出具有最大preV的prevVal。 否则，Proposer可以设置任意的val。   上面的例子中，因为两条Promise消息的最大之前提议号都是5，所以P2只能发送Accept(6，1)。\nA2 A3之前并没有承诺丢弃提议号6，所以会回复Accepted消息。\n以下是完整的算法流程：\n1，Proposer\n发送Prepare(n)消息给众数个acceptors，n必须满足\n 唯一性 大于其他任何已经尝试过的提议号（Proposal Number）  2，Acceptor\n当Acceptor接收到一条Prepare(n)消息时，会根据是否对提议号n作出过承诺而作出不同的反应：\n 如果已经承诺过了，丢弃这条消息 如果没有承诺，则询问我之前作出过其他承诺了吗？  如果Yes，则回复Proposer一条Promise(n, (之前承诺的n, 之前接受的val)) 如果NO，则回复Proposer一条Promise(n)的承诺消息，消息的含义是我会承诺丢弃掉所有小于n的消息。    3，Proposer\n当Proposer接收到来自众数节点的对提议号n的Promise消息后，发送Accept(n, val)给Acceptor，n是提议号，val是:\n 如果Proposer接收到任何的Promise(n, (prevN, prevVal))，它必须从中选出具有最大preV的prevVal。 否则，Proposer可以设置任意的val。  4，Acceptor\n当一个Acceptor接收到Accept(n, val)小时的时候，会询问自己是否承诺过丢弃n?\n 如果Yes，则丢弃这条消息 如果NO，则回复Accepted(n, val)消息给Proposer，同时将Accepted(n, val)发送给所有的Learner节点。  算法分析 #  上一篇提到过共识算法的三个特性：\n 终止(Termination): 所有的正确的进程最终会决定好一个值。 共识 (Agreement): 所有的正确的进程最终会在一个相同的值上面达成共识。 有效性(Validity): 达成共识的值必须是之前提议过的。  三个特性是无法同时满足的，那么Paxos在哪一点上做了妥协呢？答案是终止性。有一种可能就是两个或多个Proposer在互相竞争，导致没有一个Proposer可以获得众数个Acceptor的承诺。\n你可能会问了，既然多个Proposer互相竞争导致算法不能终止，那为什么我们只允许一个Proposer呢？\n一个原因是容错，单点模型不利于容错，这个很容易理解。另一个重要的原因是，如果一个Proposer终止运行了， 那么就要重新选举出一个Proposer，而选举算法用的又是Paxos，ummm。所以我们不能只允许一个Proposer存在因为这样的共识算法会变成依赖于共识算法。\n其实也有其他共识算法会将Leader选举作为算法的一个阶段，比如Raft，这个后面会做介绍。\nMultiPaxos #  我们之前都在谈论如何在单个值上面达成共识，但实际应用中，更多的是对多个值，或者说是一连串的值达成共识。\n比如之前提到的全序广播(Totally Ordered BroadCast)，其中的一个问题是如何保证每个进程会按照一样的顺序进行消息交付，这就是对一系列值达成共识的例子。\n可能有同学会想，可不可以多次执行单值的Paxos算法去实现多值的共识呢？我们发现Paxos达成共识需要发送大量的网络消息，如果多次运行Paxos必然会导致效率的低下。\n我们再回顾下达成共识的里程碑。\n实际上在M1阶段后，有众数个Acceptors做出了承诺，我们完全可以一直进行accept()和accepted()操作，直到单个的Proposer崩溃了。\n上面的就是MultiPaxos，上面的操作是安全的如果P1一直没有崩溃的话。但是如果其他的Proposer出现了呢？其实不影响，如果新出现的Proposer P2发送了Prepare(6)，那么P1的后续消息也就被丢弃了，此时又降级到了普通的Paxos，但其实这样的情况很少发生。\n容错 #  最多多少的acceptors崩溃后系统能够继续正常运行？\n如果f是你希望容忍的acceptors失败，那么就需要2f+1个acceptors.\n最多多少的Proposers崩溃后系统能够继续正常运行？\nf+1，系统是至少需要有一个Proposer。\n"});})();